## Baseline 2
# train_baseline2:
# 	python3 train_baseline.py \
# 	  --model_name_or_path google/flan-t5-${SIZE} \
# 	  --tokenizer_name google/flan-t5-${SIZE} \
# 	  --config_name google/flan-t5-${SIZE} \
# 	  --output_dir models/checkpoints/flan-t5-${SIZE}-extra-prefix \
# 	  --max_p_length 256 \
# 	  --max_q_length 16 \
# 	  --per_device_train_batch_size 4 \
# 	  --m_negative_per_example 2 \
# 	  --m_positive_per_example 2 \
# 	  --learning_rate 1e-4 \
# 	  --evaluation_strategy steps \
# 	  --max_steps 10000 \
# 	  --save_steps 2000 \
# 	  --eval_steps 500 \
# 	  --train_file ${TRAIN_FILE} \
# 	  --baseline_prefix "Generate a question with relevance label <extra_id_{0}> for the passage: {1}" \
# 	  --prefix_tuning true \
# 	  --do_train \
# 	  --do_eval 

# generate_baseline2:
# 	python3 generate.py \
# 	  --model_path models/checkpoints/flan-t5-base-extra-prefix/checkpoint-2000 \
# 	  --model_name google/flan-t5-base \
# 	  --input_jsonl ${EVAL_FILE} \
# 	  --output_jsonl results/baseline2_prefix.jsonl \
# 	  --device cuda:1 \
# 	  --batch_size 8 \
# 	  --max_p_length 256 \
# 	  --num_pred 11 \
# 	  --prefix "Generate a question with relevance label <extra_id_{0}> for the passage: {1}"
