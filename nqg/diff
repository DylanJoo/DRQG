26a27
>     num_labels: int = field(default=1)
31,32c32,33
<     adaptive_pooling: Optional[str] = field(default=None)
<     n_soft_prompts: int = field(default=1)
---
>     n_prompts: int = field(default=1)
>     n_labels: int = field(default=1)
34a36,37
>     annealing_fn: str = field(default='logistic')
>     # for logistic/linear annealing
37,49d39
<     annealing_fn: str = field(default='logistic')
<     freeze_LM: bool = field(default=True)
<     freeze_embeds: bool = field(default=True)
<     freeze_a_layer: bool = field(default=True)
<     freeze_cross_attn: bool = field(default=True)
<     initialize_from_vocab: bool = field(default=True)
<     used_prompt: Optional[str] = field(default=None)
<     n: int = field(default=1)
<     n_side: int = field(default=None)
<     add_attentive_pooler: bool = field(default=False)
<     disable_dropout: bool = field(default=False)
<     # random_masking_ratio: Optional[float] = field(default=None)
<     add_classification_head: bool = field(default=False)
52a43,49
>     # other model args
>     freeze_LM: bool = field(default=True)
>     used_prompt: Optional[str] = field(default=None)
>     used_label: Optional[str] = field(default=None)
>     add_classification_head: bool = field(default=False)
>     head_size: int = field(default=64)
>     learnable_prior: bool = field(default=False)
101,108c98
<     config = AutoConfig.from_pretrained(hfmodel_args.config_name)
< 
<     if model_args.disable_dropout:
<         config.activation_dropout=0
<         config.attention_dropout=0
<         config.classif_dropout=0
<         config.classifier_dropout=0
<         config.dropout=0
---
>     config = AutoConfig.from_pretrained(hfmodel_args.config_name, num_labels=hfmodel_args.num_labels)
113,114c103,104
<     from models import T5VQG, BartVQG
<     MODELS = {"t5": T5VQG, 'bart': BartVQG}
---
>     from models import T5VQG, BartVQG, BartCVQG
>     MODELS = {"t5": T5VQG, 'bart': BartCVQG}
126,135c116,122
<         print('Used prompt index:', model_args.used_prompt_idx)
< 
<     # Model: Enc-Dec
<     # if model_args.used_condition:
<     #     model_args.used_condition_idx = tokenizer.encode(
<     #             [str(i) for i in model_args.used_condition], 
<     #             add_special_tokens=False
<     #     )
<     #     model_args.used_condition = True
<     #     print('Used conditional index:', model_args.used_condition_idx)
---
>     
>     if model_args.used_label:
>         model_args.used_label_idx = tokenizer.encode(
>                 model_args.used_label,
>                 add_special_tokens=False
>         )
>         model_args.used_label = True
141a129,130
> 
>     # Model: Initialization
143c132,134
<     model.enc_prompts.set_embeddings()
---
>     # model.enc_lprompt.set_embeddings()
>     model.enc_iwprompts.set_embeddings()
>     model.encdec_cvae.set_embeddings()
151d141
<                 max_length=data_args.max_q_length
160,167c150,151
<     optimized_prefix = ['embed_tokens']
< 
<     if model_args.freeze_embeds is False:
<         optimized_prefix.append('shared')
<     if model_args.freeze_a_layer is False:
<         optimized_prefix.append('encoder.layers.0') # first
<     if model_args.freeze_cross_attn is False:
<         optimized_prefix.append('encoder_attn')
---
>     optimized_prefix = ['encdec_', 'enc_']
>     not_optimized_prefix = ['label_embeds']
173c157
<                 print('param {}: {}'.format(name, param.grad))
---
>                 print('Param (O) {}: {}'.format(name, param.grad))
177a162,165
>             if any([p in name for p in not_optimized_prefix]):
>                 print('Param (X) {}: {}'.format(name, param.grad))
>                 param.requires_grad = False
> 
219,220c207,208
<     from trainers import TrainerForVQG
<     trainer = TrainerForVQG(
---
>     from trainers_test import TrainerForCVQG
>     trainer = TrainerForCVQG(
